In the previous chapter (chapter \ref{RL}), we manually define three types of ideal dialogue properties, i.e., ease of answering, informativeness, and coherence, 
based on which a reinforcement learning system is trained. 
However, 
it is widely
acknowledged that  
manually defined reward functions can't possibly
cover all crucial aspects and can lead to suboptimal generated utterances. 
This relates to two important questions in dialogue learning:  
what are the crucial
aspects that define an ideal conversation and how
can we quantitatively measure them.

A good dialogue model should generate utterances indistinguishable from human dialogues.
Such a goal suggests a training objective 
resembling the idea of the Turing test \cite{turing1950computing}.
We borrow the idea of adversarial training \cite{goodfellow2014generative,denton2015deep} in 
computer
vision, in which we jointly train two models, 
a generator (which takes the form of the neural  \sts model) that defines the probability of generating a dialogue sequence, and 
a discriminator
that labels dialogues as human-generated or machine-generated. 
This discriminator  is analogous to  the evaluator in the Turing test.
We cast the task as a reinforcement learning problem, in which the quality of machine-generated utterances is measured by its ability to fool the discriminator into believing that it is a human-generated one. The output from the discriminator is used as a reward to the generator, pushing it to generate 
utterances indistinguishable from human-generated dialogues. 

The idea of a Turing test---employing an evaluator 
to distinguish  machine-generated texts from human-generated
ones---can be applied not only to training but also testing,
where it goes by the name of adversarial evaluation. Adversarial
evaluation was first employed in \newcite{bowman2015generating} to evaluate sentence generation quality, and preliminarily studied in the context of dialogue generation by \newcite{kannan}. 
Here, we discuss potential pitfalls of adversarial evaluations and 
  necessary steps to avoid them and make evaluation reliable. 



Experimental results demonstrate that our approach
produces more interactive, interesting, and non-repetitive responses than standard
\sts models trained using the MLE objective function.
\section{Adversarial Reinforcement Learning}

In this section, we describe in detail the components of the proposed adversarial reinforcement learning model. 
The problem can be framed as follows: given a dialogue history $x$ consisting of a sequence of dialogue utterances,\footnote{We approximate the dialogue history using the concatenation of two preceding utterances. We found that using more than 2 context utterances yields very tiny performance improvements for \sts models.} the model needs to generate a response $y=\{y_1,y_2,...,y_{L_y}\}$.
 We view the 
 process of sentence generation 
   as a sequence of actions that are taken according to a policy defined by an
encoder-decoder recurrent neural networks.

\begin{comment}
The learning system consists of two agents. 
We use $p$ to denote sentences generated from the first agent and  $q$ to denote sentences from the second. The two agents take turns talking with each other. A dialogue can be represented as an alternating sequence of sentences generated by the two agents: $U=\{p_1, q_1, p_2, q_2, ..., p_{i}, q_{i}\}$.  We view the generated sentences as actions that are taken according to a policy defined by an
encoder-decoder recurrent neural networks.
\end{comment}
\subsection{Adversarial REINFORCE}
The adversarial REINFORCE algorithm consists of two components:
a generative model $G$ and a discriminative model $D$.

\paragraph{Generative Model} The generative model $G$ defines the
 policy that generates a response $y$ given dialogue history $x$. 
 \begin{comment}
  , namely $p_{i}=\{w_1,w_2,...,w_N\}$, 
  given the dialogue history, denoted $\pi(p_{i}|p_{i-1},q_{i-1})$.\footnote{ We found that using more than 2 context yields very tiny performance boost.
We therefore approximate the dialogue history using the concatenation of two preceding utterances. }
\end{comment}
It takes a  form similar to \sts models, which first map the source input to a vector representation using a recurrent net
and then compute the probability of generating each token in the target using a softmax function.
\begin{comment}
\begin{equation}
\begin{aligned}
&\pi(p_{i}|p_{i-1},q_{i-1})=\\
&~~~~~~~~~\prod_{w_t\in p_{i}} \pi(w_t|w_1,...,w_{t-1},p_{i-1},q_{i-1})
\end{aligned}
\label{eq2}
\end{equation}
\end{comment}

\paragraph{Discriminative Model} The discriminative model $D$ is a binary classifier that takes as input a sequence of dialogue utterances $\{x,y\}$
 and outputs a label indicating whether the input is generated by humans or machines.  
The input dialogue is encoded into a vector representation 
using a hierarchical encoder  \cite{li2015hierarchical,serban2016building},\footnote{To be specific, each utterance $p$ or $q$ is mapped to a vector representation $h_p$ or $h_q$ using LSTM \cite{hochreiter1997long}.
Another LSTM is put on sentence level, mapping the entire dialogue sequence to a single representation} which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machine-generated dialogue
(denoted $Q_-(\{x,y\})$)
 or a human-generated dialogue  (denoted $Q_+(\{x,y\})$). 
\paragraph{Policy Gradient Training}
 The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. We use policy gradient methods to achieve such a goal, in which the 
score of current utterances being human-generated ones assigned by the discriminator
(i.e.,  $Q_+(\{x,y\})$)
 is used as a reward for the generator, which is trained to maximize the expected reward of generated utterance(s) using the REINFORCE algorithm \cite{williams1992simple}:
 \begin{equation}
 J(\theta)=\mathbb{E}_{y\sim p(y|x)}(Q_+(\{x,y\})|\theta)
 \label{lb1}
 \end{equation}
Given the input dialogue history $x$, the  bot generates a dialogue utterance $y$ by sampling from the policy. 
The concatenation of the generated utterance $y$ and the input $x$ is fed to the discriminator. 
\begin{comment}
For multi-turn simulation, the second bot updates the dialogue history $x$ by adding 
the previous generated utterance, then samples another sequence $y'$  from the policy given the updated dialogue history $x$. 
The two bots take turns talking to each other (i.e.,
doing encoding and decoding) until the maximum number of simulation turns is reached. 
The discriminator then assigns a score to the entire simulated dialogue.
 \end{comment}
 The gradient of \eqref{lb1} is approximated using the likelihood ratio trick \cite{williams1992simple,glynn1990likelihood,aleksand1968stochastic}:
% \begin{equation}
%\begin{aligned}
%\nabla J(\theta)\approx& [Q_+(\{X,Y\})-b(\{X,Y\})] \nabla\log \pi(Y|X) \\
%=& [Q_+(\{X,Y\})-b(\{X,Y\})] \nabla\sum_t \log p(y_t|X,Y_{1:t-1})
%\end{aligned}
%\label{partial}
%\end{equation}
 \begin{multline}
\nabla J(\theta)\approx [Q_+(\{x,y\})-b(\{x,y\})] \\ \nabla\log \pi(y|x)
\\ = [Q_+(\{x,y\})-b(\{x,y\})] \\ \nabla\sum_t \log p(y_t|x,y_{1:t-1})
\label{partial}
\end{multline}
where $\pi$ denotes the probability of the generated responses. 
 $b(\{x,y\})$ denotes the baseline value to reduce the variance of the estimate while keeping it unbiased.\footnote{
 Like \newcite{ranzato2015sequence}, 
 we train another neural network model (the critic) to estimate the value (or future reward) of current state (i.e., the dialogue history) under the current policy $\pi$. The critic network takes as input the dialogue history, transforms it to a vector representation using a hierarchical network and maps the representation to a scalar. The network is optimized based on the mean squared loss between the estimated reward and the real reward.} The discriminator is simultaneously updated with the human generated dialogue that contains dialogue history $x$ as a positive example and the machine-generated dialogue as a negative example. 
\subsection{Reward for Every Generation Step (REGS)}
The  REINFORCE algorithm
described has the disadvantage that the expectation of the reward is approximated by only one sample, and the reward associated with this sample 
(i.e.,  $[Q_+(\{x,y\})-b(\{x,y\})]$ in Eq\eqref{partial})
is used for all actions (the generation of each token) in the generated sequence. 
Suppose, for example, the input history is {\it what's your name}, the human-generated response is {\it I am John}, and the machine-generated response is {\it I don't know}. 
The vanilla REINFORCE model assigns the same negative reward to all tokens within the human-generated response (i.e., {\it I}, {\it don't}, {\it know}), whereas 
proper credit assignment in training would  give separate rewards, most likely a neutral reward for the token {\it I}, and negative rewards to {\it don't} and {\it know}. 
We call this {\it reward for every generation step}, abbreviated {\it REGS}.

Rewards for intermediate steps or partially decoded sequences are thus necessary. Unfortunately, the discriminator is trained to assign scores to fully generated sequences, but not partially decoded ones. 
We propose two strategies for computing intermediate step rewards by (1) using Monte Carlo (MC) search and (2) training a discriminator that is able to assign rewards to partially decoded sequences.

In (1) Monte Carlo search, given a partially decoded $s_P$, the model 
keeps sampling tokens from the distribution
 until the decoding finishes. Such a process is repeated $N$ (set to 5) times and the $N$ generated   sequences will share  a common prefix $s_P$.
These $N$ sequences 
are fed to the discriminator, the average score of which is used as a reward for the $s_P$. A similar strategy is adopted in \newcite{yu2016seqgan}. 
The downside of MC is that 
it requires repeating the sampling process for each prefix of each sequence  and is thus significantly time-consuming.\footnote{Consider one target sequence with length 20, we need to sample 5*20=100 full sequences to get rewards for all intermediate steps. Training one batch with 128 examples roughly takes roughly 1 min on a single GPU, which is computationally intractable considering the size of the dialogue data we have. We thus parallelize the sampling processes, distributing jobs across 8 GPUs. }


In (2), we directly train a discriminator that is able to assign rewards to both fully and partially decoded sequences. 
We break 
the generated sequences into partial sequences, namely 
$\{y^+_{1:t}\}_{t=1}^{N_{Y^+}}$ and $\{y^-_{1:t}\}_{t=1}^{N_{Y^-}}$
 and use 
 all instances in  $\{y^+_{1:t}\}_{t=1}^{N_{Y^+}}$ as positive examples and  instances  $\{y^-_{1:t}\}_{t=1}^{N_{Y^-}}$ as negative examples. 
The problem with such a strategy is that earlier actions in a sequence are 
shared among multiple training examples for
 the discriminator 
(for example, token $y^+_1$ is contained in all partially generated sequences, which results in overfitting. 
To mitigate this problem, 
we adopt a  strategy similar to when training value networks in  {\it AlphaGo} \cite{silver2016mastering}, in which 
for each collection of subsequences of $Y$, we randomly sample only one example from $\{y^+_{1:t}\}_{t=1}^{N_{Y^+}}$ and one example from $\{y^-_{1:t}\}_{t=1}^{N_{Y^-}}$, which are treated as positive and negative examples
to update the discriminator. 
Compared with the Monte Carlo search model, this strategy is significantly more time-effective, but comes with the weakness that the discriminator becomes less accurate  after partially decoded sequences are added in as training examples. 
We find that the MC model performs better when training time is less of an issue. 


For each partially-generated sequence $Y_t=y_{1:t}$, the discriminator gives a classification score $Q_+(x,Y_t)$. 
We compute the baseline $b(x,Y_t)$
using a similar model  to the vanilla REINFORCE model. 
This yields the following gradient to update the generator:
 \begin{multline}
\nabla J(\theta)\approx \sum_t  (Q_+(x,Y_t)-b(x,Y_t))  \\
\nabla\log p(y_t|x,Y_{1:t-1})
\label{action}
\end{multline}
Comparing \eqref{action} with \eqref{partial}, we can see that
the values for  
rewards and baselines are different among generated tokens in the same response. 

\paragraph{Teacher Forcing}
Practically, we find that  updating the generative model only using Eq.~\ref{lb1} leads to  unstable training for both vanilla Reinforce and REGS, with the perplexity value skyrocketing after training the model for a few hours (even when the generator is initialized using a pre-trained \sts model). The reason this happens is that the generative model can only be indirectly exposed to the gold-standard target sequences through the reward 
passed back from the
 discriminator, and this reward is used to promote or discourage its (the generator's) own generated sequences. Such a training strategy is fragile: once the generator (accidentally) deteriorates in some training batches and the discriminator consequently does an extremely good job in recognizing sequences from the generator,  
the generator immediately gets lost. It knows that its generated sequences are bad based on the rewards outputted from the discriminator, but it does not know what sequences are good and how to push itself to generate these good sequences (the odds of generating a good response from random sampling are minute, due to the vast size of the space of possible sequences). Loss of the reward signal leads to a breakdown in the training process.

To alleviate this issue and give the generator more direct access to the gold-standard targets, 
we propose also feeding human generated responses to the generator for model updates.  
The most straightforward strategy is for the discriminator to automatically assign a reward of 1 (or other positive values) to the human generated responses and for
the generator to use this reward to update itself on human generated examples. 
This can be seen as having a teacher intervene with the generator some fraction of the time and force it to generate the true responses, 
an approach that is similar to the professor-forcing algorithm of \newcite{lamb2016professor}.

A closer look reveals that 
this modification is the same as the standard training of \sts models, making the final training
alternately update the \sts model using the adversarial objective and the MLE objective. One can think of the 
professor-forcing 
model as a regularizer to 
regulate the generator once it starts deviating from the training dataset.

We also propose another workaround, in which the discriminator first assigns a reward to a human generated example using its own model,  and the generator then updates itself using this reward on the human generated example only if the reward is larger than the baseline value. Such a strategy has the advantage that different weights for model updates are assigned to different human generated examples (in the form of different reward values produced by the generator) and that human generated examples are always associated with non-negative weights. 

A review of the proposed model is shown in Figure \ref{fig:adver-reinforce}. 
\begin{figure}
\small
\line(1,0){220} \\
{\bf For} number of training iterations {\bf do} \\
.~\hspace{0.3cm}{\bf For} i=1,D-steps {\bf do} \\
.~\hspace{0.8cm}Sample (X,Y) from real data \\
.~\hspace{0.8cm}Sample $\hat{Y}\sim G(\cdot|X)$\\
.~\hspace{0.8cm} Update $D$ using $(X,Y)$ as positive examples and $(X,\hat{Y})$ as negative examples. \\
.~\hspace{0.3cm}{\bf End} \\
.\\
.~\hspace{0.2cm} {\bf For} i=1,G-steps {\bf do} \\
.~\hspace{0.8cm}Sample (X,Y) from real data \\
.~\hspace{0.8cm}Sample $\hat{Y}\sim G(\cdot|X)$ \\
.~\hspace{0.8cm}Compute Reward $r$ for $(X,\hat{Y})$ using $D$.\\
.~\hspace{0.8cm}Update $G$  on $(X,\hat{Y})$ using reward $r$\\
.~\hspace{0.8cm}Teacher-Forcing: Update $G$  on $(X,Y)$\\
.~\hspace{0.3cm}{\bf End} \\
{\bf End} \\
\line(1,0){220}
\caption{A brief review of the proposed adversarial reinforcement algorithm
for training the generator $G$ and discriminator $D$.
The reward $r$ from the discriminator $D$ can be computed using different strategies according to whether using REINFORCE or REGS.  
The update of the generator $G$ on $(X,\hat{Y})$ can be done by either using Eq.\ref{partial} or Eq.\ref{action}.
D-steps is set to 5 and G-steps is set to 1.}
\label{fig:adver-reinforce}
\end{figure}

\subsection{Training Details}
We first  pre-train the generative model by predicting target sequences given the dialogue history. We trained a  \sts model
  \cite{sutskever2014sequence}   
 with an attention mechanism \cite{bahdanau2014neural,luong2015effective}  on the OpenSubtitles dataset. 
 We followed protocols recommended by \newcite{sutskever2014sequence}, such as gradient clipping, mini-batch, and learning rate decay.
We also pre-train the discriminator. To generate negative examples, 
we  decode   part of the training data. 
Half of the negative examples are generated using beam-search with mutual information reranking 
as described in \newcite{li2015diversity},
and the other half is generated from sampling. 




\begin{comment}
We add policy entropy $\nabla H(Y|X)$ to regularize the model  \cite{williams1991function,mnih2016asynchronous}. 
We also add an historical averaging term \cite{brown1951iterative,salimans2016improved} as a regularizer to smooth training:
 \begin{equation}
L= ||\theta-\frac{1}{t}\sum_{t}\theta_t ||^2
 \end{equation}
where $\theta_t$ denotes the value of the parameters at past time $t$. The historical average of the parameters is updated after each parameter update. 
\end{comment}

For 
data processing, 
model training, and decoding  (both the proposed adversarial training model and the standard \sts models), we employ a few  strategies that  improve response quality, including: 
(1) Remove training examples with the length of responses shorter than a threshold (set to 5).
We find that this significantly improves the general response quality.\footnote{To compensate for the loss of short responses, one can train a separate model using short sequences.}
(2) Instead of using the same learning rate for all examples, using a weighted learning rate that considers the average tf-idf score for tokens within the response.
Such a strategy
 decreases the influence from dull and generic utterances.\footnote{We treat each sentence as a document. Stop words are removed.
 Learning rates are normalized within one batch. 
 For example, suppose $t_1$, $t_2$, ..., $t_i$, ... ,$t_N$ denote the tf-idf scores for  sentences within current batch and $lr$ denotes the original learning rate. The learning rate for sentence with index $i$ is $N\cdot lr\cdot \frac{t_i}{\sum_{i'}t_{i'}}$. 
 To avoid exploding learning rates for sequences with extremely rare words, the tf-idf score of a sentence 
 is capped at $L$ times the minimum tf-idf score in the current batch. $L$ is empirically chosen and is set to 3.
  } (3) Penalizing intra-sibling ranking when doing beam search decoding to promote N-best list diversity as described in Chapter 3.  (4) Penalizing word types (stop words excluded) that have already been generated. Such a strategy dramatically decreases the rate of repetitive responses such as {\it no. no. no. no. no.} or contradictory responses such as {\it I don't like oranges but i like oranges}. 

\section{Adversarial Evaluation}
In this section, we discuss details of strategies for successful adversarial evaluation. 
It is worth noting that 
 the proposed adversarial training and adversarial evaluation are separate procedures.They are independent of each other and share no common parameters. 


 The idea of adversarial evaluation, first proposed by 
\newcite{bowman2015generating}, is to 
train a discriminant
function
 to separate  generated and
true sentences, in an attempt to evaluate the model's sentence generation capability.
The idea has been preliminarily studied by \newcite{kannan} in the context of dialogue generation. 
Adversarial evaluation  also resembles the idea of the Turing test,
which requires a human evaluator 
to distinguish  machine-generated texts from human-generated ones. 
Since it is time-consuming and costly to 
 ask a human to talk to a model 
and give judgements,
we  train a machine evaluator
in place of the  human evaluator to distinguish the human dialogues and machine dialogues, and we use it to measure 
 the general quality of the generated responses. 

Adversarial evaluation involves both  training and testing. 
At training time, the evaluator is trained to
label dialogues as machine-generated (negative) or  human-generated (positive). 
At test time, the trained evaluator is evaluated on a held-out dataset.
If the human-generated dialogues and machine-generated ones are indistinguishable, the model  will achieve 50 percent accuracy at test time.  
\subsection{Adversarial Success}
We define Adversarial Success ({\it AdverSuc} for short) to be the fraction of instances in which a model is capable of fooling the evaluator. {\it AdverSuc} is the difference between 1 and the accuracy 
achieved by the  evaluator. Higher values of {\it AdverSuc} for a dialogue generation model are better. 
\begin{comment}
Existing evaluation metrics---e.g., word-overlap metrics such as BLEU and METEOR, or the recently proposed dialogue evaluation metric ADEM \cite{hey}, which learns to predict human evaluation score to input responses---all focus on single turn dialogue quality evaluation. We argue that a dialogue generation model can be more properly evaluated on multi-turn dialogues. 
This is comparable to  the Turing test, in which the human evaluator is asked to chat with the machine for five minutes rather than just a single turn. 
We therefore propose evaluating the model
using multi-turn dialogues generated 
 by having  two bots  talk with each other. 
 \end{comment}
\begin{comment}
We  report {\it AdverSuc} scores
 on dialogues with different numbers of simulated turns (length), specifically, {\it AdverSuc-1}, {\it AdverSuc-2} and {\it AdverSuc-3}, where {\it AdverSuc-N} denotes
the 
{\it AdverSuc} score
 of an evaluator trained on dialogues 
 between the two bots
 simulated for $N$ turns. Theoretically, the larger $N$ is, the easier it is for the discriminator to recognize a machine-generated dialogue and thus the lower the {\it AdverSuc} score should be. 
 \end{comment}
\subsection{Testing the Evaluator's Ability}
One caveat  with the adversarial evaluation methods is that 
they are model-dependent. 
We approximate the human evaluator in the Turing test with an automatic evaluator and assume that the evaluator is perfect: low accuracy of the discriminator should indicate high quality of the responses, since we interpret this to mean the generated responses are indistinguishable from the human ones. Unfortunately, there is another factor that can lead to low discriminative accuracy: a poor discriminative model. 
 Consider a  discriminator that always gives random labels or always gives the same label.
 Such an evaluator 
   always yields a high {\it AdverSuc} value of 0.5. 
   \newcite{bowman2015generating} propose  two different discriminator models 
separately using {\it unigram} features and {\it neural} features. It is hard to tell which feature set is more reliable. 
The standard strategy of testing the model on a held-out development set is not suited to this case, since a model that overfits the development set is necessarily superior. 
% ~~\\~~\\

To deal with this issue, we propose setting up a few manually-invented situations to test the ability of the automatic evaluator.
 This is akin to setting up  examinations to test the ability of the human evaluator in the Turing test. 
We  report not only the {\it AdverSuc} values,  but also the scores that the evaluator
achieves
 in these manually-designed test cases, indicating how much we can trust the reported {\it AdverSuc}. 
 We develop scenarios in which we know  in advance how a perfect evaluator should behave, and then compare   {\it AdverSuc}  from a discriminative model with the gold-standard  {\it AdverSuc}. Scenarios we design include:
 \begin{tightitemize}
 \item We use human-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an {\it AdverSuc} of $0.5$ (accuracy $50\%$), which is the gold-standard result.
  \item We use machine-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an {\it AdverSuc} of $0.5$ (accuracy $50\%$).
 \item We use original human-generated dialogues as positive examples and dialogues 
 consisting of
  random utterances 
  as negative examples. A perfect evaluator should give an {\it AdverSuc} of $0$ (accuracy $100\%$).
 \item We use original human-generated dialogues as positive examples and dialogues with 
 responses replaced by the utterances immediately following the true responses. 
 A perfect evaluation should give an {\it AdverSuc} of $0$ (accuracy $100\%$).
\end{tightitemize}
The {\it evaluator reliability error} (ERE) is the average deviation of an evaluator's adversarial error
 from the gold-standard error in the above tasks, with equal weight for each task. 
The smaller the error, the more reliable the evaluator is. 

%\begin{table*}[!htb]
\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{11cm}}\hline
{\bf Input}&tell me ... how long have you had this falling sickness ?\\
{\bf ~~Vanilla-MLE}&i 'm not a doctor .\\
{\bf ~~Vanilla-Sample}&well everything you did was totally untrue .\\
{\bf ~~REINFORCE}&i don 't know how long it 's been .\\
{\bf ~~REGS Monte Carlo}& A few months, I guess .\\\hline
%{\bf Input}&he 's been repeating that name ever since we found him .\\
%{\bf ~~Vanilla-MLE}&he 's been in the same room since the last time .\\
%{\bf ~~Vanilla-Sample}&not as bruised as not from sitting back and forth by the man you saw on the island .\\
%{\bf ~~REINFORCE}&he 's not a murderer .\\
%{\bf ~~Monte Carlo}&he 's a witness to the charges .\\\hline
{\bf Input}&so i had the doctors test sammy 's response to conditioning .\\
{\bf ~~Vanilla-MLE}&sammy wrote the test sammy wrote the test .\\
{\bf ~~Vanilla-Sample}&objects pick objects up objects objects objects objects objects \\
{\bf ~~REINFORCE}&i 'm not sure that 's relevant .\\
{\bf ~~REGS Monte Carlo}&so he took the pills .\\\hline
{\bf Input}&what are you going to do ?\\
{\bf ~~Vanilla-MLE}&i 'm going to the movies .\\
{\bf ~~Vanilla-Sample}&get him outta here first !\\
{\bf ~~REINFORCE}&i 'm going to get you .\\
{\bf ~~REGS Monte Carlo}&i 'm going to get you a new car .\\\hline
{\bf Input}&they fear your power your intellect .\\
{\bf ~~Vanilla-MLE}&you 're the only one who knows what 's going on .\\
{\bf ~~Vanilla-Sample}&when they are conquered and you surrender they will control all of us .\\
{\bf ~~REINFORCE}&i 'm afraid i 'm not ready yet .\\
{\bf ~~REGS Monte Carlo}&i 'm not afraid of your power .\\\hline
%{\bf Input}&you just shot an unarmed man .\\
%{\bf Vanilla-MLE}&he 's armed with armed robbery .\\
%{\bf Vanilla-Sample}&he 's tired of himself coming to life .\\
%{\bf REINFORCE}&i 'm not a cop .\\
%{\bf Monte Carlo}&you 're not going to shoot me ?\\\hline
\end{tabular}
\caption{Sampled responses from different models. More in Appendix Tables \ref{example-appendix1} and \ref{example-appendix2}.}
\label{example-appendix1}
\end{table*}


\subsection{Machine-vs-Random Accuracy}
Evaluator reliability error uses scenarios constructed from human-generated
dialogues to assess feature or hyper-parameter choice for the evaluator. Unfortunately, no machine-generated responses are involved in the ERE metric. 
The following example illustrates the serious weakness resulting from this strategy:
as will be shown in the experiment section, 
when inputs are decoded using greedy or beam search models,  
most generation systems to date yield an adversarial success less than 10 percent (evaluator accuracy 90 percent). 
But when using sampling for decoding, the adversarial success skyrockets to around 40 percent,\footnote{Similar results are also reported in \newcite{kannan}.} only 10 percent less than what's needed to pass the Turing test. 
A close look at the decoded sequences using sampling tells a different story: the responses from sampling are sometimes incoherent, irrelevant or even ungrammatical. 

We thus propose an additional sanity check, in which we report the accuracy of distinguishing between machine-generated responses and randomly sampled responses ({\it machine-vs-random} for short). 
This resembles the N-choose-1 metric described in \newcite{shao15}. 
Higher accuracy indicates that the generated responses are distinguishable from randomly sampled human responses, indicating that the generative model is not fooling the generator simply by introducing randomness. 
As we will show in Sec.~\ref{sec:experiments}, using sampling results in high {\it AdverSuc} values but low {\it machine-vs-random} accuracy. 
\begin{comment}
\subsection{A Special Issue: Sequence Length}
It is widely accepted that
\sts models have a extremely strong bias towards generating short responses \cite{sountsov2016length}, and it is usually hard to generate coherent and meaningful long responses \cite{shao15}. 
The length of the sequence is thus a feature that might dominate the evaluator.\footnote{Only using this single feature, a evaluator can achieve an accuracy of roughly 75 percent.} 
On one hand, we acknowledge the inferiority of algorithms to date and the incapability of generating long, coherent responses;
on the other hand, we also want to compare different models
and evaluate 
 different aspects of responses such as coherence, meaningfulness, etc. 
Excluding the effect of this predominant feature is thus favorable in comparing different models. 
In our experimental section, we will report the discriminative success both including and excluding the effect of sequence length. 
\end{comment}

\section{Experimental Results} \label{sec:experiments}
In this section, 
we detail experimental results on adversarial success and human evaluation.  

\begin{table}[htb]
\centering
\small
\begin{tabular}{ccc}\toprule
Setting&ERE\\
SVM+Unigram&0.232\\
Concat Neural &0.209 \\
Hierarchical Neural &0.193 \\
SVM+Neural+multil-features&0.152 \\\bottomrule
\hline
\end{tabular}
\caption{ERE scores obtained by different models.}
\label{ERE}
\end{table} 


\subsection{Adversarial Evaluation}
\paragraph{ERE} We first test 
 adversarial evaluation models with different 
 feature sets and 
 model architectures for reliability, as measured by evaluator reliability error (ERE).
 We explore the following models:
   (1) {\it SVM+Unigram}:  SVM using unigram features.\footnote{Trained using the SVM-Light package \cite{joachims2002learning}.}
 A multi-utterance dialogue (i.e., input messages and responses) is transformed to a unigram representation; (2) 
{\it Concat Neural}: 
a neural classification model with 
a softmax function that takes as input the concatenation of representations of constituent dialogues sentences;
 (3) {\it Hierarchical Neural}: 
  a hierarchical encoder    
  with a  structure similar to the discriminator used in the reinforcement; and
  (4) 
  {\it SVM+Neural+multi-lex-features}: 
  a SVM model that uses the following features: unigrams,  neural representations of dialogues obtained by the neural model trained using strategy (3),\footnote{The representation before the softmax layer.} the forward likelihood $\log p(y|x)$ and backward likelihood $p(x|y)$.

ERE scores obtained by different models are reported in Table \ref{ERE}. 
As can be seen, the {\it hierarchical neural} evaluator (model 3) is more reliable than simply concatenating the sentence-level representations (model 2).
Using the combination of neural features and lexicalized features yields the most reliable evaluator. 
For the rest of this section, we report results obtained 
by the 
{\it hierarchical Neural} setting due to its end-to-end nature, despite its inferiority to {\it SVM+Neural+multil-features}. 

Table \ref{adv} presents
 {\it AdverSuc} values for different models, along with {\it machine-vs-random} accuracy described in Section 4.3. 
Higher values of  {\it AdverSuc}  and  {\it machine-vs-random} are better. 

Baselines we consider include standard \sts models using greedy decoding ({\it MLE-greedy}), beam-search ({\it MLE+BS}) and sampling, as well as the 
mutual information reranking model (as described in Chapter 3) with two algorithmic variations: (1) MMI+$p(y|x)$, in which a large N-best list is first generated using a pre-trained \sts model and then reranked by the backward probability $p(x|y)$ and (2) MMI$-p(y)$, in which language model probability is penalized during decoding. 

Results are shown in Table \ref{adv}. What first stands out  is decoding using sampling (as discussed in Section 4.3), achieving a significantly higher {\it AdverSuc} number than all the rest models. 
However, this does not indicate the superiority of the sampling decoding model, since the {\it machine-vs-random} accuracy is at the same time significantly lower. This means that sampled responses based on \sts models are not only hard for an evaluator to distinguish from real human responses, but also from randomly sampled responses.
A similar, though much less extreme, effect is observed for MMI$-p(y)$, which has an {\it AdverSuc} value slightly higher than {\it Adver-Reinforce}, but a significantly lower {\it machine-vs-random} score. 

By comparing different baselines, we find that MMI+$p(y|x)$ is better than {\it MLE-greedy}, which is in turn better than {\it MLE+BS}. This result is in line with human-evaluation results from \newcite{li2015diversity}. 
The two proposed adversarial algorithms achieve better performance than the baselines. We expect this to be the case, since the adversarial algorithms are trained on an objective function 
more similar to the
the evaluation metric (i.e., adversarial success). 
{\it REGS} performs slightly better than the vanilla REINFORCE algorithm. 

\begin{table}
\small
\centering
\begin{tabular}{ccc}\toprule
Model&{\it AdverSuc}&{\it machine-vs-random} \\\midrule
MLE-BS&0.037&0.942 \\
MLE-Greedy&0.049&0.945 \\ 
MMI+$p(t|s)$&0.073&0.953\\
MMI-$p(t)$&0.090& 0.880\\
Sampling&0.372&0.679\\
Adver-Reinforce&0.080&0.945 \\
Adver-REGS&0.098&0.952\\\bottomrule
\end{tabular}
\caption{{\it AdverSuc} and {\it machine-vs-random} scores achieved by different training/decoding strategies.}
\label{adv}
\end{table}

\begin{comment}
According to the {\it evaluator reliability error} shown in the first row of Table \ref{adver}, the neural evaluator is generally more reliable than the unigram evaluator. Also, the 
evaluator  becomes more reliable as the number of simulated turns increases. 

The proposed adversarial  model yields better adversarial error scores than the  baselines. 
Interestingly, we find that {\it sampling} generally performs  better than beam search. 
This is similar to phenomenon reported in \newcite{li2015diversity} that greedy decoding yields better results than beam-search. 
All  models perform poorly on {\it adver-3} evaluation metric, with the best adversarial error being 0.422 (the trained evaluator is able to  distinguish between human-generated and machine-generated dialogues with greater than 90 percent accuracy for all models), indicating that none of the models we have to date is capable of simulating conversations longer than three turns.

We observe a performance boost from the {\it two-turn} adversarial model over the {\it single-turn} model. 
The explanation is as follows: dull, generic responses are commonly found in the training set. If we are doing single-turn simulation, it might confuse the discriminator in terms of whether to label the dull responses as positive or negative. 
 However, it is very rare that the two consecutive turns in a normal conversation are both dull and generic. The  discriminator in the two-turn simulation can thus more confidently assign negative rewards to the commonplace simulated instances. As the reward is back-propagated to the generative model, the probability of generating dull responses will be pushed down for both turns.

We do not see a significant performance boost yielded by the {\it three-turn} training strategy 
over the 
 {\it two-turn} strategy. The performance even deteriorates as the number of the simulated turns surpasses three. We suspect that this is because: (1) The number of actions   grows linearly with the number the simulated turns. As the sequence of actions becomes longer, the system becomes increasingly hard to train.
(2) The discriminator is too much more powerful than the generator as the number of the simulated turns becomes larger. The results for {\it four-turn} simulation are thus omitted for brevity.  \todo{(WM: maybe list 4-turn numbers anyway?)}
\end{comment}
\begin{comment}
\subsection{N-choose-1 Metric}
We evaluate the model using an N-choose-1 ranking metric, as described in \newcite{shao15}. 
 In
the N-choose-1 evaluation, given dialogue history, we ask the generator to score N candidate responses, where one of the N candidate responses is the ground truth and the rest are random responses.
N-choose-1 accuracy is defined to be the fraction of test cases in which the trained model assigns the highest score to the ground truth.
We report 50-, 10-, and 2-choose-1 accuracy. Results are shown in Table \ref{choose}. 
Again we see a clear performance boost from the adversarial model over the
standard \sts variants.

\begin{table}
\small
\centering
\begin{tabular}{cccc}
\hline
Model&50&10&2\\\hline
\sts&0.087&0.28&0.73 \\
MMI&0.43&0.69&0.88\\
Adversarial&0.53&0.75&0.95\\\hline
\end{tabular}
\caption{N-choose-1 evaluation on different models.}
\label{choose}
\end{table}
\end{comment}

\subsection{Human Evaluation}
For human evaluation, we follow protocols defined in Chapter 5, employing
crowdsourced judges to evaluate a random sample of
200 items. We present both an input message and the
generated outputs to 3 judges and ask them to decide
which of the two outputs is better ({\it single-turn}
general quality). 
They were instructed to prefer outputs that were more specific (relevant) to the message and preceding context,
Ties are permitted. Identical
strings are assigned the same score. 
We also
 present the judges with {\it multi-turn}
conversations simulated between the two agents. Each conversation
consists of 3 turns.
Results are presented in Table \ref{human}.
We observe a significant quality improvement on both 
single-turn quality and multi-turn quality from the proposed adversarial model.
It is worth noting that the reinforcement learning system described in Chapter 5, which 
simulates conversations between two bots and 
is trained based on manually designed reward functions, only improves multi-turn dialogue quality, while the model described in this chapter improves both single-turn and multi-turn dialogue generation quality. 
This confirms that the reward adopted in adversarial training is more general, natural, and effective in training dialogue systems.


\begin{table}
\small
\centering
\begin{tabular}{cccc}\\\toprule
Setting &adver-win &adver-lose &tie\\\midrule
single-turn& 0.62& 0.18 &0.20 \\
multi-turn& 0.72 &0.10& 0.18\\\bottomrule
\end{tabular}
\caption{The gain
from the proposed adversarial model
 over the mutual information system
based on pairwise human judgments.}
\label{human}
\end{table}

\begin{comment}
\begin{table}
\centering
\small
\begin{tabular}{l}\toprule
{\bf Input}: {Do you love me?} (1) \\\midrule
{\it Mutual information reranking} \\ 
{Yes, of course}. (2)\\
 {I love you too.} (3)\\
{Goodbye, honey}. (4) \\\midrule
{\it Adversarial training} \\
More than anything in this world. (2) \\
Then we can stay together, you and me. (3) \\
Yes, we can. (4) \\\hline
{\bf Input}: {Where are you going?} (1) \\\midrule
{\it Mutual information reranking} \\ 
I am going to the restroom. (2) \\
See you later. (3) \\
See you later. (4)\\\midrule
{\it Adversarial training} \\
I'm going to join the Army. (2)\\
 You're joking! (3)\\
 No I'm not. (4)\\\bottomrule
\end{tabular}	
\caption{
Multi-turn dialogue simulation. The first turn (index 1) is input by the authors. Then the two agents take turns conversing, taking as input the other agent's prior generated turn.}
\label{multi}
\end{table}
\end{comment}


\begin{table*}[!htb]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{8cm}}\toprule
{\bf Input}&maybe his eyes were too big for his stomach .\\
{\bf Vanilla-MLE}&he 's got a big mouth .\\
{\bf Vanilla-Sample}&i 'm sure he 's unhappy with the fact .\\
{\bf Reinforce}&he 's a little bit of a pain .\\
{\bf MC}&i don 't think so .\\\midrule
{\bf Input}&they didn 't have much success with this problem commander .\\
{\bf Vanilla-MLE}&they 're not the only ones who have been in the system .\\
{\bf Vanilla-Sample}&it 's the new priority only .\\
{\bf Reinforce}&they 're not the only ones who can help us .\\
{\bf MC}&can we find someone else ?\\\midrule
{\bf Input}&and he carried them all the time wherever he went .\\
{\bf Vanilla-MLE}&i don 't know who he is .\\
{\bf Vanilla-Sample}&the church ... suffered the winds of 900 meters with horse aid .\\
{\bf Reinforce}&he was a good man .\\
{\bf MC}&he was a very good man .\\\midrule
{\bf Input}&somebody probably flushed their pet snake down the toilet .\\
{\bf Vanilla-MLE}&i 'm sure the dog will be here soon .\\
{\bf Vanilla-Sample}&i could at least think .\\
{\bf Reinforce}&i don 't think so .\\
{\bf MC}&i don 't think so .\\\midrule
{\bf Input}&excuse me but how did we pay for these suits ?\\
{\bf Vanilla-MLE}&i 'm not a beggar .\\
{\bf Vanilla-Sample}&is this the money eugene ?\\
{\bf Reinforce}&i don 't know what to do .\\
{\bf MC}& how much money you owe me ?\\\midrule
{\bf Input}&... while the beautiful and happy world was madly destroyed ;\\
{\bf Vanilla-MLE}&i was born in the world of the world\\
{\bf Vanilla-Sample}&as in childhood i took you to one tenth of a postcard\\
{\bf Reinforce}&and the world was full of joy .\\
{\bf MC}&and the world was full of joy .\\\midrule
{\bf Input}&i was kinda hoping that you were my dad .\\
{\bf Vanilla-MLE}&i 'm just a little bit of a little bit of a little bit of a little bit of a\\
{\bf Vanilla-Sample}&shiny gold gold and silver in the mountains .\\
{\bf Reinforce}&i 'm not gonna be a baby .\\
{\bf MC}&i 'm not going to be a father .\\\midrule
{\bf Input}&the man who does that is not afraid to die .\\
{\bf Vanilla-MLE}&he 's not afraid of the dark .\\
{\bf Vanilla-Sample}&even the program is the only truth\\
{\bf Reinforce}&he 's also not afraid of me .\\
{\bf MC}&he 's such a brave man !\\\bottomrule
\end{tabular}
\caption{Sampled responses outputted from different models}
\label{example-appendix1}
\end{table*}


\begin{table*}[!htb]
\centering
\footnotesize
\begin{tabular}{p{4cm}p{8cm}}\toprule

{\bf Input}&i 'll have to drive home .\\
{\bf Vanilla-MLE}&i 'll be back in a minute .\\
{\bf Vanilla-Sample}&let 's go for some fun .\\
{\bf Reinforce}&i 'll drive you home .\\
{\bf MC}&i don 't have a car .\\\midrule
{\bf Input}&i had to work late and then i went for a walk .\\
{\bf Vanilla-MLE}&i was in the middle of the road .\\
{\bf Vanilla-Sample}&my body was just pounding .\\
{\bf Reinforce}&i was just trying to help you .\\
{\bf MC}&i was just trying to get a little sleep .\\\midrule
{\bf Input}&lots of cream lots of sugar .\\
{\bf Vanilla-MLE}&i 'm gonna be in the kitchen .\\
{\bf Vanilla-Sample}&was this gay or red ?\\
{\bf Reinforce}&i don 't want to be a vegetable .\\
{\bf MC}&i 'm not a very good cook .\\\midrule
{\bf Input}&so they just let him go ?\\
{\bf Vanilla-MLE}&he 's not gonna let them go through the motions .\\
{\bf Vanilla-Sample}&they asked him whether we could go see ernesto ?\\
{\bf Reinforce}&i don 't know what to do .\\
{\bf MC}&he 's not going to be here any longer .\\\midrule
{\bf Input}&you must not bomb this town .\\
{\bf Vanilla-MLE}&you have to go back to the city .\\
{\bf Vanilla-Sample}&the one is a multiple choice .\\
{\bf Reinforce}&I think I have to .\\
{\bf MC}&I think I have to .\\\midrule
{\bf Input}&communism recently failed in eastern europe .\\
{\bf Vanilla-MLE}&the war is not over .\\
{\bf Vanilla-Sample}&there are people who hope that in tokyo they won 't risk losing their departing .\\
{\bf Reinforce}&and the people who are not the same .\\
{\bf MC}&the war is still going on .\\\midrule
{\bf Input}&you should stop in more often .\\
{\bf Vanilla-MLE}&I don 't think so .\\
{\bf Vanilla-Sample}&is it possible as old as you ?\\
{\bf Reinforce}&i 'm not going to be a burden on you .\\
{\bf MC}&i don 't want to be a burden .\\\hline{\bf Input}&he couldn 't have heard us .\\
{\bf Vanilla-MLE}&he was in the middle of the night  .\\
{\bf Vanilla-Sample}&he should have been somewhere out here .\\
{\bf Reinforce}&he 's not a good guy .\\
{\bf MC}&he 's got a lot of nerve.\\\midrule
{\bf Input}&that 's our wake up call .\\
{\bf Vanilla-MLE}&we 're gonna be late for the meeting .\\
{\bf Vanilla-Sample}&c 'mon c 'mon let 's get outta here .\\
{\bf Reinforce}&i 'll be right back .\\
{\bf MC}&we 're gonna have to get to the station .\\
\bottomrule
\end{tabular}
\caption{Sampled responses outputted from different models}
\label{example-appendix2}
\end{table*}

\section{Conclusion }
In this chapter, drawing  intuitions from the Turing test, we propose using an adversarial training approach for response generation. We cast the model in the framework of reinforcement learning and train a generator based on the signal from a discriminator to generate response sequences indistinguishable from human-generated dialogues.
We observe clear performance improvements on multiple metrics from the adversarial training strategy.

